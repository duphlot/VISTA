{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67954200",
   "metadata": {},
   "source": [
    "# VISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82686fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\">\n",
       "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
       "</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\n",
    "\"\"\"\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26cfb76",
   "metadata": {},
   "source": [
    "## Environment Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143310fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y clip\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install decord\n",
    "!pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from llm.base import AgentClient\n",
    "from data.cache.memory_handler import MessageMemoryHandler\n",
    "import chainlit as cl\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from data.prompts.video_analysis_agent import VIDEO_ANALYSIS_AGENT_PROMPT\n",
    "from utils.basetools.video_analysis_tool import create_video_analysis_tool\n",
    "import clip\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import cv2\n",
    "from google.colab import drive, files\n",
    "\n",
    "load_dotenv()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "print(f\"CLIP model loaded on {device}\")\n",
    "\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "DATA_DIR = Path(\"/content/drive/MyDrive/soICT/datasets\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "HIDDEN_DIM = 512\n",
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a6d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Google Colab, make using_colab = True\n",
    "using_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f6142",
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "\n",
    "    !mkdir images\n",
    "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n",
    "\n",
    "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f62f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "print(f\"CLIP model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f840b7",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9675670",
   "metadata": {},
   "source": [
    "### Cutting Frame From input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dcf5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.basetools.preprocess_video import VideoPreprocessor\n",
    "\n",
    "uploaded = files.upload() \n",
    "video_filename = list(uploaded.keys())[0]\n",
    "sample_video_path = Path(video_filename)\n",
    "\n",
    "\n",
    "output_dir = Path(f\"/content/keyframes_output/{sample_video_path.stem}\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "video_preprocessor_enhanced = VideoPreprocessor(frame_interval=1, similarity_threshold=0.8)\n",
    "\n",
    "if sample_video_path.exists():\n",
    "    print(f\"Processing video: {sample_video_path}\")\n",
    "    video_info = video_preprocessor_enhanced.get_video_info(str(sample_video_path))\n",
    "    print(f\"Video info: {video_info}\")\n",
    "\n",
    "    filtered_frames, selected_indices = video_preprocessor_enhanced.extract_keyframes_with_redundancy_removal(\n",
    "        str(sample_video_path),\n",
    "        max_frames=video_info['frame_count']\n",
    "    )\n",
    "\n",
    "    print(f\"Selected {len(filtered_frames)} keyframes\")\n",
    "\n",
    "    for i, frame in enumerate(filtered_frames):\n",
    "        frame_path = output_dir / f\"frame_{i:04d}.jpg\"\n",
    "        cv2.imwrite(str(frame_path), cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"Saved keyframes to {output_dir}\")\n",
    "else:\n",
    "    print(f\"Video not found at: {sample_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdde6de",
   "metadata": {},
   "source": [
    "### Masking and Detect Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5018b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.basetools.mask import process_frame, show_processed_frames\n",
    "\n",
    "processed_images = [process_frame(frame) for frame in filtered_frames]\n",
    "show_processed_frames(processed_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b85b3",
   "metadata": {},
   "source": [
    "### Call Agent for Generate Relation between 2 next to Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.agents.image_relation_agent import ImageRelationAgent, ImageRelationInput, RelationOutput\n",
    "# --- Usage ---\n",
    "agent = ImageRelationAgent()\n",
    "\n",
    "all_frame_relations = {}\n",
    "for i, frame in enumerate(processed_images):\n",
    "    input_data = ImageRelationInput(\n",
    "        mask_frame=frame,\n",
    "        original_img=filtered_frames[i], \n",
    "        prev_objects=agent.prev_objects\n",
    "    )\n",
    "    result = agent.run(input_data)\n",
    "    all_frame_relations[f\"frame_{i}\"] = result.relations\n",
    "\n",
    "for frame_name, rels in all_frame_relations.items():\n",
    "    print(f\"{frame_name}:\")\n",
    "    for r in rels:\n",
    "        print(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca0207",
   "metadata": {},
   "source": [
    "### Call Agent for Building Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b427da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.agents.scene_graph_agent import SameEntityAgent, SceneGraphInput\n",
    "\n",
    "same_entity_agent = SameEntityAgent()\n",
    "\n",
    "scene_graph_input = SceneGraphInput(frames_dict=all_frame_relations)\n",
    "linked_result = same_entity_agent.run(scene_graph_input)\n",
    "\n",
    "linked_scene_graph = linked_result.combined_relations\n",
    "\n",
    "for r in linked_scene_graph:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72603369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.agents.graph_reasoning_agent import GraphReasoningAgent, GraphReasoningInput\n",
    "\n",
    "graph_text_for_question = \"\\n\".join(linked_scene_graph)\n",
    "\n",
    "question = \"YOUR QUESTION\"\n",
    "\n",
    "agent = GraphReasoningAgent()\n",
    "\n",
    "input_data = GraphReasoningInput(\n",
    "    question=question,\n",
    "    graph_text=graph_text_for_question\n",
    ")\n",
    "\n",
    "result = agent.run(input_data)\n",
    "answer = result.answer\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
