{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duphlot/soICT/blob/main/notebooks/automatic_mask_generator_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa21d44",
      "metadata": {
        "id": "5fa21d44"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7c0041e",
      "metadata": {
        "id": "b7c0041e"
      },
      "source": [
        "# Automatically generating object masks with SAM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289bb0b4",
      "metadata": {
        "id": "289bb0b4"
      },
      "source": [
        "Since SAM can efficiently process prompts, masks for the entire image can be generated by sampling a large number of prompts over an image. This method was used to generate the dataset SA-1B.\n",
        "\n",
        "The class `SamAutomaticMaskGenerator` implements this capability. It works by sampling single-point input prompts in a grid over the image, from each of which SAM can predict multiple masks. Then, masks are filtered for quality and deduplicated using non-maximal suppression. Additional options allow for further improvement of mask quality and quantity, such as running prediction on multiple crops of the image or postprocessing masks to remove small disconnected regions and holes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "072e25b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 41
        },
        "id": "072e25b8",
        "outputId": "0550fb8c-d86b-4ea6-d6a2-fda888de6670"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\">\n",
              "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
              "</a>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(\n",
        "\"\"\"\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\"\"\"\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0b71431",
      "metadata": {
        "id": "c0b71431"
      },
      "source": [
        "## Environment Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47e5a78f",
      "metadata": {
        "id": "47e5a78f"
      },
      "source": [
        "If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fe300fb",
      "metadata": {
        "id": "4fe300fb"
      },
      "outputs": [],
      "source": [
        "using_colab = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0685a2f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0685a2f5",
        "outputId": "718013f1-3c12-49fb-fe66-b052194ebeef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "Torchvision version: 0.23.0+cu126\n",
            "CUDA is available: True\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-uz3xisb1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-uz3xisb1\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment_anything\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36592 sha256=8c049effe7d03c0890683410f4719aa79222750148d7ead85c2a12d6e4ec231a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z0knq6bx/wheels/29/82/ff/04e2be9805a1cb48bec0b85b5a6da6b63f647645750a0e42d4\n",
            "Successfully built segment_anything\n",
            "Installing collected packages: segment_anything\n",
            "Successfully installed segment_anything-1.0\n",
            "--2025-09-17 13:54:14--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99846 (98K) [image/jpeg]\n",
            "Saving to: ‘images/dog.jpg’\n",
            "\n",
            "dog.jpg             100%[===================>]  97.51K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-09-17 13:54:14 (3.94 MB/s) - ‘images/dog.jpg’ saved [99846/99846]\n",
            "\n",
            "--2025-09-17 13:54:14--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.225.143.54, 13.225.143.109, 13.225.143.122, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.225.143.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_h_4b8939.pth’\n",
            "\n",
            "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   173MB/s    in 16s     \n",
            "\n",
            "2025-09-17 13:54:31 (152 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if using_colab:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "    !mkdir images\n",
        "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n",
        "\n",
        "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd2bc687",
      "metadata": {
        "id": "fd2bc687"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y clip\n",
        "!pip install git+https://github.com/openai/CLIP.git\n"
      ],
      "metadata": {
        "id": "i8PSFgDcLppl",
        "outputId": "89ddf68f-82ea-4ab0-c723-4f440ca42947",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "i8PSFgDcLppl",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: clip 1.0\n",
            "Uninstalling clip-1.0:\n",
            "  Successfully uninstalled clip-1.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-m45o74k0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-m45o74k0\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.23.0+cu126)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=7cd7f0ead48f2cdf181e6a8101127db67fa0a22391e231e9cfb75a502140a87d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dddinklb/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50"
      ],
      "metadata": {
        "id": "-4ntxAiPKD7v"
      },
      "id": "-4ntxAiPKD7v",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import clip\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/soICT/datasets\")\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "HIDDEN_DIM = 512\n",
        "NUM_HEADS = 8\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"CLIP imported successfully\")\n",
        "print(clip.__file__)  # should point to site-packages/CLIP/__init__.py\n",
        "\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "print(f\"CLIP model loaded on {device}\")\n"
      ],
      "metadata": {
        "id": "AUrxH9ocKF-C",
        "outputId": "6cda6da4-2525-491e-af47-a493abbc2c79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AUrxH9ocKF-C",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "CLIP imported successfully\n",
            "/usr/local/lib/python3.12/dist-packages/clip/__init__.py\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:03<00:00, 92.7MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP model loaded on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c937160",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c937160",
        "outputId": "7b6a5580-d413-4048-9550-92c155d4956a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-_3np7t_2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-_3np7t_2\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit a1ce2f956a1d2212ad672e3c47d53405c2fe4312\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.0.10)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.19.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-25.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (8.2.1)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (4.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.74.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.9)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-25.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp312-cp312-linux_x86_64.whl size=6733294 sha256=656f033b174fe1524f9b5fc43201550991ec2cb3ab3ef9dbd1ec4a27da283b72\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sh980zvm/wheels/d3/6e/bd/1969578f1456a6be2d6f083da65c669f450b23b8f3d1ac14c1\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=e11c31659f55c80e843d67b9f8683281e9b8220b9e5536684587828f0b964e9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "Successfully built detectron2 fvcore\n",
            "Installing collected packages: yacs, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed black-25.1.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 pathspec-0.12.1 portalocker-3.2.0 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict, Any\n",
        "from PIL import Image\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class VideoPreprocessor:\n",
        "    \"\"\"Extract keyframes from videos with CLIP embedding and improved redundancy removal\"\"\"\n",
        "\n",
        "    def __init__(self, frame_interval: int = 5, similarity_threshold: float = 0.9, max_recent: int = 5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            frame_interval: Extract every N frames\n",
        "            similarity_threshold: Cosine similarity threshold for removing redundant frames\n",
        "            max_recent: Number of recent selected frames to compare for redundancy\n",
        "        \"\"\"\n",
        "        self.frame_interval = frame_interval\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.max_recent = max_recent\n",
        "\n",
        "    def extract_frames(self, video_path: str, max_frames: int = 100) -> List[np.ndarray]:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        frame_count = 0\n",
        "        extracted_count = 0\n",
        "\n",
        "        while cap.isOpened() and extracted_count < max_frames:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if frame_count % self.frame_interval == 0:\n",
        "                # frame = cv2.resize(frame, (224, 224))\n",
        "                frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "                extracted_count += 1\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "    def get_clip_embedding(self, frame: np.ndarray) -> np.ndarray:\n",
        "        pil_image = Image.fromarray(frame)\n",
        "        image_input = clip_preprocess(pil_image).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            image_features = clip_model.encode_image(image_input)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        return image_features.cpu().numpy().flatten()\n",
        "\n",
        "    def remove_redundant_frames(self, frames: List[np.ndarray], min_frame_gap: int = 5) -> Tuple[List[np.ndarray], List[int]]:\n",
        "        \"\"\"\n",
        "        Remove redundant frames based on CLIP embeddings with temporal spacing.\n",
        "\n",
        "        Args:\n",
        "            frames: list of frames (np.ndarray)\n",
        "            min_frame_gap: minimum number of frames between kept frames (temporal spacing)\n",
        "\n",
        "        Returns:\n",
        "            filtered_frames: list of selected frames\n",
        "            selected_indices: indices of selected frames\n",
        "        \"\"\"\n",
        "        if not frames:\n",
        "            return [], []\n",
        "\n",
        "        embeddings = [self.get_clip_embedding(f) for f in frames]\n",
        "        embeddings = np.array(embeddings)\n",
        "\n",
        "        selected_frames = [frames[0]]\n",
        "        selected_indices = [0]\n",
        "        recent_embeddings = [embeddings[0:1]]\n",
        "\n",
        "        for i in range(1, len(frames)):\n",
        "            current_emb = embeddings[i:i+1]\n",
        "            similarities = [cosine_similarity(current_emb, e)[0][0] for e in recent_embeddings]\n",
        "\n",
        "            time_since_last_kept = i - selected_indices[-1]\n",
        "\n",
        "            # Keep frame if similarity low OR enough frames passed since last kept\n",
        "            if all(s < self.similarity_threshold for s in similarities) or time_since_last_kept >= min_frame_gap:\n",
        "                selected_frames.append(frames[i])\n",
        "                selected_indices.append(i)\n",
        "                recent_embeddings.append(current_emb)\n",
        "                if len(recent_embeddings) > self.max_recent:\n",
        "                    recent_embeddings.pop(0)\n",
        "\n",
        "        return selected_frames, selected_indices\n",
        "\n",
        "\n",
        "    def extract_keyframes_with_redundancy_removal(self, video_path: str, max_frames: int = 100) -> Tuple[List[np.ndarray], List[int]]:\n",
        "        raw_frames = self.extract_frames(video_path, max_frames)\n",
        "        if not raw_frames:\n",
        "            return [], []\n",
        "        filtered_frames, selected_indices = self.remove_redundant_frames(raw_frames)\n",
        "        return filtered_frames, selected_indices\n",
        "\n",
        "    def get_video_info(self, video_path: str) -> Dict[str, Any]:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        duration = frame_count / fps if fps > 0 else 0\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        cap.release()\n",
        "        return {'fps': fps, 'frame_count': frame_count, 'duration': duration, 'width': width, 'height': height}\n",
        "\n",
        "video_preprocessor_enhanced = VideoPreprocessor(\n",
        "    frame_interval=1,\n",
        "    similarity_threshold=0.97\n",
        ")\n",
        "from pathlib import Path\n",
        "width_video = 0\n",
        "height_video = 0\n",
        "sample_video_path = Path(\"/content/drive/MyDrive/soICT/datasets/4010069381.mp4\")\n",
        "if sample_video_path.exists():\n",
        "    print(f\"Testing enhanced video preprocessing with: {sample_video_path}\")\n",
        "    video_info = video_preprocessor_enhanced.get_video_info(str(sample_video_path))\n",
        "    print(f\"Video info: {video_info}\")\n",
        "    print(f\"Total frames in video: {video_info['frame_count']}\")\n",
        "    width_video = video_info['width']\n",
        "    height_video = video_info['height']\n",
        "\n",
        "    filtered_frames, selected_indices = video_preprocessor_enhanced.extract_keyframes_with_redundancy_removal(\n",
        "        str(sample_video_path),\n",
        "        max_frames=video_info['frame_count']\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFinal result:\")\n",
        "    print(f\"- Selected {len(filtered_frames)} frames\")\n",
        "    print(f\"- Selected frame indices: {selected_indices}\")\n",
        "    if filtered_frames:\n",
        "        print(f\"- Each frame shape: {filtered_frames[0].shape}\")\n",
        "    # for i, frame in enumerate(filtered_frames):\n",
        "    #     plt.imshow(frame)\n",
        "    #     plt.title(f\"Keyframe {i}\")\n",
        "    #     plt.axis(\"off\")\n",
        "    #     plt.show()\n",
        "\n",
        "else:\n",
        "    print(f\"Sample video not found at: {sample_video_path}\")\n"
      ],
      "metadata": {
        "id": "3WeUSRsZLZ3i",
        "outputId": "f3af096c-8d24-4feb-e147-1a41050e559e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3WeUSRsZLZ3i",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing enhanced video preprocessing with: /content/drive/MyDrive/soICT/datasets/4010069381.mp4\n",
            "Video info: {'fps': 29.97002997002997, 'frame_count': 369, 'duration': 12.3123, 'width': 640, 'height': 480}\n",
            "Total frames in video: 369\n",
            "\n",
            "Final result:\n",
            "- Selected 102 frames\n",
            "- Selected frame indices: [0, 2, 5, 7, 8, 11, 12, 13, 14, 16, 21, 26, 29, 34, 39, 44, 49, 54, 59, 64, 69, 74, 78, 82, 87, 92, 95, 99, 102, 107, 112, 116, 119, 124, 126, 131, 135, 138, 139, 143, 147, 149, 154, 159, 164, 169, 174, 179, 182, 187, 189, 192, 197, 202, 205, 206, 210, 215, 220, 225, 229, 231, 236, 240, 242, 247, 250, 255, 259, 262, 264, 267, 270, 272, 276, 281, 284, 288, 289, 292, 297, 302, 307, 312, 317, 322, 324, 329, 332, 337, 342, 346, 351, 356, 360, 361, 363, 364, 365, 366, 367, 368]\n",
            "- Each frame shape: (480, 640, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, torch, numpy as np\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
        "\n",
        "# --- Load SAM ---\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "# --- Load Detectron2 panoptic model ---\n",
        "cfg = get_cfg()\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\n",
        "    \"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"\n",
        "))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
        "    \"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"\n",
        ")\n",
        "\n",
        "# cfg.merge_from_file(\"configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
        "# cfg.MODEL.WEIGHTS = \"detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/13951456/model_final_c10459.pkl\"\n",
        "cfg.MODEL.DEVICE = device\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# --- Utility ---\n",
        "def iou(mask1, mask2):\n",
        "    inter = np.logical_and(mask1, mask2).sum()\n",
        "    union = np.logical_or(mask1, mask2).sum()\n",
        "    return inter / union if union > 0 else 0\n",
        "\n",
        "# --- Run on one frame ---\n",
        "frame = cv2.imread(\"/content/01.jpeg\")\n",
        "\n",
        "# Panoptic prediction\n",
        "outputs = predictor(frame)\n",
        "panoptic_seg, segments_info = outputs[\"panoptic_seg\"]\n",
        "panoptic_seg = panoptic_seg.cpu().numpy()\n",
        "\n",
        "# SAM prediction\n",
        "sam_masks = mask_generator.generate(frame)\n",
        "sam_masks = [m[\"segmentation\"] for m in sam_masks]\n",
        "\n",
        "# Refine masks\n",
        "refined_masks = []\n",
        "for seg in segments_info:\n",
        "    seg_id = seg[\"id\"]\n",
        "    mask_pan = (panoptic_seg == seg_id)\n",
        "\n",
        "    best_iou, best_sam = 0, None\n",
        "    for sam_m in sam_masks:\n",
        "        score = iou(mask_pan, sam_m)\n",
        "        if score > best_iou:\n",
        "            best_iou, best_sam = score, sam_m\n",
        "\n",
        "    if best_iou > 0.5:\n",
        "        mask_final = best_sam  # dùng biên từ SAM\n",
        "    else:\n",
        "        mask_final = mask_pan  # giữ nguyên\n",
        "\n",
        "    refined_masks.append({\n",
        "        \"category_id\": seg[\"category_id\"],\n",
        "        \"isthing\": seg[\"isthing\"],\n",
        "        \"mask\": mask_final\n",
        "    })\n"
      ],
      "metadata": {
        "id": "_5MYwS21_-oq",
        "outputId": "f1fb1c35-cb7f-45fe-8856-42a0f77143f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_5MYwS21_-oq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_cafdb1.pkl: 261MB [00:14, 17.5MB/s]                           \n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = predictor(frame)\n",
        "panoptic_seg, segments_info = outputs[\"panoptic_seg\"]\n"
      ],
      "metadata": {
        "id": "aBCq37wjBfHN"
      },
      "id": "aBCq37wjBfHN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "v = Visualizer(frame[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]))\n",
        "out = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n",
        "\n",
        "cv2.imwrite(\"panoptic_result.jpg\", out.get_image()[:, :, ::-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOOi5LzuBdyB",
        "outputId": "664a24a7-7021-4067-8127-9b008d9d8e2a"
      },
      "id": "QOOi5LzuBdyB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator\n"
      ],
      "metadata": {
        "id": "yTP_AIuO9ww_",
        "outputId": "174d1694-e84f-4964-fd33-f2e0bfeec3fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yTP_AIuO9ww_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.12/dist-packages (from deep-translator) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from deep-translator) (2.32.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.8.3)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from pycocotools import mask as mask_utils\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# lấy metadata từ Detectron2 panoptic model\n",
        "metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
        "thing_classes = metadata.thing_classes\n",
        "stuff_classes = metadata.stuff_classes\n",
        "\n",
        "json_results = []\n",
        "\n",
        "for obj in refined_masks:\n",
        "    # encode mask sang RLE\n",
        "    rle = mask_utils.encode(\n",
        "        np.asfortranarray(obj[\"mask\"].astype(np.uint8))\n",
        "    )\n",
        "    rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "    cat_id = int(obj[\"category_id\"])\n",
        "    if obj[\"isthing\"]:\n",
        "        cat_name = thing_classes[cat_id]\n",
        "    else:\n",
        "        cat_name = stuff_classes[cat_id]\n",
        "\n",
        "    json_results.append({\n",
        "        \"category_id\": cat_id,\n",
        "        \"category_name\": cat_name,\n",
        "        \"isthing\": bool(obj[\"isthing\"]),\n",
        "        \"segmentation\": rle\n",
        "    })\n",
        "\n",
        "with open(\"results.json\", \"w\") as f:\n",
        "    json.dump(json_results, f, indent=2)\n"
      ],
      "metadata": {
        "id": "IGstHWS-7fa0"
      },
      "id": "IGstHWS-7fa0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "\n",
        "client = genai.Client(api_key=\"AIzaSyCS0bljIt691Tsl4mSFhEX0BRhlpAovxNE\")\n",
        "\n",
        "with open(\"panoptic_result.jpg\", \"rb\") as f:\n",
        "    img_bytes = f.read()\n",
        "\n",
        "image_file = client.files.upload(file=Path(\"panoptic_result.jpg\"))\n",
        "\n",
        "prompt = \"\"\"\n",
        "Identify all objects in the image and describe their pairwise relations.\n",
        "Rules:\n",
        "1. If multiple objects share the same name (e.g., multiple persons), assign them indices: person1, person2, person3, etc.\n",
        "2. Use the format: <object1> - <relation> - <object2>\n",
        "3. Only output relations, no extra sentences.\n",
        "\n",
        "Examples:\n",
        "person1 - holding - umbrella1\n",
        "person2 - sitting_on - chair1\n",
        "dog1 - next_to - dog2\n",
        "Response with translate the relations and object into Vietnamese. (Including person1 -> người1, umbrella1 -> ô1, chair1 -> ghế1, dog1 -> chó1, dog2 -> chó2,....)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=[\n",
        "        image_file,prompt\n",
        "    ]\n",
        ")\n",
        "\n",
        "# In ra caption\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.text:\n",
        "        print(part.text)"
      ],
      "metadata": {
        "id": "Fr6fL1p8Hyxs"
      },
      "id": "Fr6fL1p8Hyxs",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}